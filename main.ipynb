{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.tenalg import multi_mode_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_video_names\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from mytypes import VideoNumpy, CoordinatesNumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конвертация видео в 3х мерный тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames_seq(path: str) -> VideoNumpy:\n",
    "\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    ret = True\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Normalization\n",
    "            frame = frame.astype(np.float32) / 255.0\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    video_tensor = np.stack(frames, axis=0)\n",
    "\n",
    "    return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 1\n",
    "video_num = 1\n",
    "video_tensor = get_frames_seq(f'data/LPW/{user}/{video_num}.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 480, 640, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение сверточной сети для извлечения признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = [str(i) for i in range(1, 23)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_path = 'data/LPW/'\n",
    "train_tensor = None\n",
    "test_tensor = None\n",
    "train_gt = None\n",
    "test_gt = None\n",
    "\n",
    "for user_id in tqdm(user_ids):\n",
    "    user_path = os.path.join(data_path, user_id)\n",
    "    video_names = get_video_names(user_path)\n",
    "\n",
    "    for video in video_names:\n",
    "        video_path = os.path.join(user_path, video + '.avi')\n",
    "        gt_path = os.path.join(user_path, video + '.txt')\n",
    "        video_tensor = get_frames_seq(video_path)\n",
    "        gt = np.genfromtxt(gt_path)\n",
    "        _train_frames, _test_frames, _train_gt, _test_gt= train_test_split(video_tensor, gt)\n",
    "        del video_tensor\n",
    "        del gt\n",
    "\n",
    "        if train_tensor is None:\n",
    "            train_tensor = _train_frames\n",
    "            test_tensor = _test_frames\n",
    "            train_gt = _train_gt\n",
    "            test_gt = _test_gt\n",
    "\n",
    "        else:\n",
    "            train_tensor = np.concatenate((train_tensor, _train_frames), axis=0)\n",
    "            test_tensor = np.concatenate((test_tensor, _test_frames), axis=0)\n",
    "            train_gt = np.concatenate((train_gt, _train_gt), axis=0)\n",
    "            test_gt = np.concatenate((test_gt, _test_gt), axis=0)\n",
    "        \n",
    "        del _train_frames, _test_frames, _train_gt, _test_gt\n",
    "\n",
    "np.save('data/train_frames.npy', train_tensor)\n",
    "np.save('data/test_frames.npy', test_tensor)\n",
    "np.save('data/train_gt.npy', train_gt)\n",
    "np.save('data/test_gt.npy', test_gt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с полученными признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = np.load('new_features.npy')\n",
    "ground_truth = np.load('ground_truth.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# features_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = scaler.fit_transform(ground_truth)\n",
    "# new_features = scaler.fit_transform(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOPLS(X: np.ndarray, \n",
    "          Y: np.ndarray, \n",
    "          R: int = None,\n",
    "          rank: tuple = [2, 4, 5, 2]\n",
    "          ) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "\n",
    "    D = []\n",
    "    G = []\n",
    "    P = []\n",
    "    Q = []\n",
    "\n",
    "    for i in tqdm(range(R)):\n",
    "        covariance_tensor = np.einsum('ijkl, im -> jklm', X, Y)\n",
    "        core, factors = tucker(covariance_tensor, rank=rank, random_state=1)\n",
    "        redeemed_tensor = multi_mode_dot(X, [np.eye(X.shape[0])] + [factor.T for factor in factors[:-1]])\n",
    "        unw_cov = redeemed_tensor.reshape(redeemed_tensor.shape[0], -1)\n",
    "\n",
    "        U, _, _ = np.linalg.svd(unw_cov, full_matrices=False)\n",
    "        t_r = U[:, 0]\n",
    "        t_r = t_r.reshape(-1,1)\n",
    "        G_x = multi_mode_dot(X, [t_r.T] + [factor.T for factor in factors[:-1]])\n",
    "        G_y = multi_mode_dot(Y, [t_r.T] + [factors[-1].T])\n",
    "        X = X - multi_mode_dot(G_x, [t_r] + factors[:-1])\n",
    "        Y = Y - multi_mode_dot(G_y, [t_r] + [factors[-1]])\n",
    "        # t.append(t_r)\n",
    "        P.append(factors[:-1])\n",
    "        Q.append(factors[-1])\n",
    "\n",
    "        G.append(G_x)\n",
    "        D.append(G_y)\n",
    "\n",
    "\n",
    "        if i % (R // 20) == 0:\n",
    "            print('new X norm', np.linalg.norm(X))\n",
    "            print('new Y norm', np.linalg.norm(Y))\n",
    "\n",
    "    return G, D, P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:00<00:02, 34.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 2032.2340221817942\n",
      "new Y norm 61.988346902993705\n",
      "new X norm 910.1339964582319\n",
      "new Y norm 26.824757675938255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:02, 36.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 709.5857445046669\n",
      "new Y norm 19.987509204192914\n",
      "new X norm 572.705582595102\n",
      "new Y norm 18.623140760544555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:00<00:01, 38.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 497.32286249205276\n",
      "new Y norm 17.849721253828005\n",
      "new X norm 469.81328535393084\n",
      "new Y norm 16.924050680665545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:01<00:01, 37.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 431.76675368084085\n",
      "new Y norm 16.293559492400156\n",
      "new X norm 411.36673626619245\n",
      "new Y norm 15.65381797214087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:01<00:01, 38.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 388.57456341620616\n",
      "new Y norm 15.152213295040726\n",
      "new X norm 366.9525342353712\n",
      "new Y norm 14.712782328121575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:01<00:00, 43.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 355.0238899846087\n",
      "new Y norm 14.246777876091963\n",
      "new X norm 341.8653137072467\n",
      "new Y norm 13.86104737608423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:01<00:00, 43.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 331.76809647786223\n",
      "new Y norm 13.56037036820479\n",
      "new X norm 321.3426539162459\n",
      "new Y norm 13.337995581370757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [00:01<00:00, 44.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 311.61403107433324\n",
      "new Y norm 13.18175971617756\n",
      "new X norm 302.99432894836167\n",
      "new Y norm 13.010105112329102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [00:02<00:00, 43.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 293.6447047649844\n",
      "new Y norm 12.882495345365948\n",
      "new X norm 287.34377260003606\n",
      "new Y norm 12.755403025915747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [00:02<00:00, 43.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new X norm 280.2122294095965\n",
      "new Y norm 12.657913744579444\n",
      "new X norm 272.9350118568515\n",
      "new Y norm 12.591602746510208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 40.98it/s]\n"
     ]
    }
   ],
   "source": [
    "G, D, P, Q = HOPLS(new_features, ground_truth, 100, rank=[1,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, G, D, P, Q):\n",
    "\n",
    "    Y = 0\n",
    "\n",
    "    for i in tqdm(range(len(P))):\n",
    "        redeemed_tensor = multi_mode_dot(X, [np.eye(X.shape[0])] + [factor.T for factor in P[i]])\n",
    "        unw_cov = redeemed_tensor.reshape(redeemed_tensor.shape[0], -1)\n",
    "\n",
    "        U, _, _ = np.linalg.svd(unw_cov, full_matrices=False)\n",
    "        t_r = U[:, 0]\n",
    "        t_r = t_r.reshape(-1,1)\n",
    "\n",
    "        Y += multi_mode_dot(D[i], [t_r] + [Q[i]])\n",
    "\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:01, 52.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 61.12it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = predict(new_features, G, D, P, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.200368855616304"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_back_gt = scaler.inverse_transform(ground_truth)\n",
    "scaled_back_prediction = scaler.inverse_transform(prediction)\n",
    "\n",
    "root_mean_squared_error(scaled_back_prediction, \n",
    "                        scaled_back_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы:\n",
    "1. Удалось достичь результата лучше чем с помощью простой CNN\n",
    "2. HOPLS чувствителен к нормализации данных перед их использованием\n",
    "3. Слишком большой ранг ведет к сильной деградации качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_tensor = get_frames_seq('data/LPW/1/1.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames, height, width, num_channels = video_tensor.shape\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_video.avi', fourcc, 20.0, (width, height))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    frame = video_tensor[i]\n",
    "\n",
    "    if frame.dtype != np.uint8:\n",
    "        frame = (frame * 255).astype(np.uint8)\n",
    "\n",
    "    x1, y1 = scaled_back_prediction[i].astype(np.uint16)\n",
    "    x2, y2 = scaled_back_gt[i].astype(np.uint16)\n",
    "    cv2.circle(frame, (x1, y1), 5, (0, 255, 0), -1)\n",
    "    cv2.circle(frame, (x2, y2), 5, (0, 0, 255), -1)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
